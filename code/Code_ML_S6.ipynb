{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución Ejercicion ML Sesión 6 ####\n",
    "<p>\n",
    "    Cargar las librerias necesarias para el codigo que sa va a realizar. La primera vez que se ejecute si se descargan las stopwords, una vez descargadas se puede comentar esa linea\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re   \n",
    "from sklearn.datasets import load_files \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import nltk\n",
    "#nltk.download('stopwords')    \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Se cargan los textos en movie_data. Esta variable ahora contiene dos campos, uno .data que es una lsita de textos y .target que indica la carpeta de la que se han extraido. Estas variables pasan a ser X e y\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = load_files(r\"./../texts\")  \n",
    "X, y = movie_data.data, movie_data.target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Se crea la lista vacia documents y se llena con los textos que se van a ir limpiando segun las expresiones regulares descritas\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):  \n",
    "    \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "      # Removing unwanted characters\n",
    "    document = re.sub(r'x9[0-9]', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Se crea el conversor que traduce cada texto a un vector que contiene la importancia de cada termino en cada posicion\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    De la matriz X se obtienen las particiones para hacer training y testing\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfVectorizer(max_features=100, min_df=1, max_df=0.9, stop_words=stopwords.words('english'))  \n",
    "tfidfconverter.fit(documents)\n",
    "X = tfidfconverter.transform(documents).toarray()\n",
    "\n",
    "#X = tfidfconverter.fit_transform(documents).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Se entrena el clasificador con los datos diseñados y se obtienen las predicciones para el conjunto de test\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Se pinta la matriz de confusion y unas medidas extras para determinar la bondad del ajuste de nuestro clasificador\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 1]\n",
      " [0 4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91         6\n",
      "           1       0.80      1.00      0.89         4\n",
      "\n",
      "   micro avg       0.90      0.90      0.90        10\n",
      "   macro avg       0.90      0.92      0.90        10\n",
      "weighted avg       0.92      0.90      0.90        10\n",
      "\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Os dejo una sección de código para que jugueis y le metais vuestras propias frases al clasificador y veais qué os sale\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a text: This is a test of a text on the economy and other topics\n"
     ]
    }
   ],
   "source": [
    "entrada = input(\"Write a text: \")\n",
    "document = re.sub(r'\\W', ' ', str(entrada))\n",
    "\n",
    "# remove all single characters\n",
    "document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "# Remove single characters from the start\n",
    "document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "# Substituting multiple spaces with single space\n",
    "document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "# Removing prefixed 'b'\n",
    "document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "  # Removing unwanted characters\n",
    "document = re.sub(r'x9[0-9]', '', document)\n",
    "\n",
    "# Converting to Lowercase\n",
    "document = document.lower()\n",
    "\n",
    "# Lemmatization\n",
    "document = document.split()\n",
    "\n",
    "document = [stemmer.lemmatize(word) for word in document]\n",
    "document = ' '.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = tfidfconverter.transform([document]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(prueba) \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    En esta celda podeis ver el vocabulario que se ha seleccionado con los parámetros que hemos metido. Echando un ojo podréis ver que hay basurilla por ahi que habría que limpiar, si os queréis entretener con ello para practicar expresiones regulares puede ser buen ejercicio\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'election': 22,\n",
       " 'business': 10,\n",
       " 'love': 47,\n",
       " 'child': 13,\n",
       " 'nwe': 64,\n",
       " 'together': 88,\n",
       " 'think': 87,\n",
       " 'big': 6,\n",
       " 'happen': 34,\n",
       " 'nation': 55,\n",
       " 'nso': 62,\n",
       " 'give': 29,\n",
       " 'good': 31,\n",
       " 'see': 77,\n",
       " 'problem': 71,\n",
       " 'hillary': 37,\n",
       " 'clinton': 16,\n",
       " 'nthe': 63,\n",
       " 'better': 5,\n",
       " 'ever': 25,\n",
       " 'look': 45,\n",
       " 'system': 81,\n",
       " 'place': 68,\n",
       " 'keep': 42,\n",
       " 'money': 52,\n",
       " 'trade': 89,\n",
       " 'deal': 18,\n",
       " 'economy': 21,\n",
       " 'worker': 95,\n",
       " 'school': 75,\n",
       " 'said': 74,\n",
       " 'young': 99,\n",
       " 'even': 24,\n",
       " 'new': 59,\n",
       " 'end': 23,\n",
       " 'interest': 39,\n",
       " 'made': 48,\n",
       " 'million': 51,\n",
       " 'government': 33,\n",
       " 'change': 12,\n",
       " 'last': 43,\n",
       " 'got': 32,\n",
       " 'policy': 70,\n",
       " 'mean': 49,\n",
       " 'republican': 73,\n",
       " 'city': 15,\n",
       " 'across': 1,\n",
       " 'china': 14,\n",
       " 'united': 91,\n",
       " 'nnow': 60,\n",
       " 'support': 80,\n",
       " 'something': 78,\n",
       " '000': 0,\n",
       " 'nbut': 57,\n",
       " 'isi': 40,\n",
       " 'security': 76,\n",
       " 'israel': 41,\n",
       " 'military': 50,\n",
       " 'part': 66,\n",
       " 'another': 2,\n",
       " 'everyone': 26,\n",
       " 'thing': 86,\n",
       " 'obama': 65,\n",
       " 'plan': 69,\n",
       " 'believe': 4,\n",
       " 'woman': 94,\n",
       " 'bring': 8,\n",
       " 'pay': 67,\n",
       " 'national': 56,\n",
       " 'law': 44,\n",
       " 'immigration': 38,\n",
       " 'border': 7,\n",
       " 'go': 30,\n",
       " 'community': 17,\n",
       " 'care': 11,\n",
       " 'trump': 90,\n",
       " 'tax': 83,\n",
       " 'vote': 92,\n",
       " 'build': 9,\n",
       " 'working': 96,\n",
       " 'future': 28,\n",
       " 'tell': 84,\n",
       " 'friend': 27,\n",
       " 'really': 72,\n",
       " 'hard': 35,\n",
       " 'done': 20,\n",
       " 'nand': 54,\n",
       " 'thank': 85,\n",
       " 'talk': 82,\n",
       " 'must': 53,\n",
       " 'help': 36,\n",
       " 'stop': 79,\n",
       " 'donald': 19,\n",
       " 'well': 93,\n",
       " 'lot': 46,\n",
       " 'applause': 3,\n",
       " 'nquestion': 61,\n",
       " 'ndonald': 58,\n",
       " 'xe2': 98,\n",
       " 'x80': 97}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfconverter.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
